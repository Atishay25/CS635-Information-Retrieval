{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":15445.016924,"end_time":"2024-10-11T23:27:43.122281","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-11T19:10:18.105357","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Define transforms for data augmentation and normalization\ntransform = transforms.Compose([transforms.Resize((224, 224)), \n                                transforms.ToTensor(), \n                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n                               ])\n\n# Load CIFAR-10 dataset\ntrainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\nnum_labels = 100\n\ndevice = 'cuda'\n\n# Create DataLoader for batch processing\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=False, num_workers=2)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=2)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":12.847175,"end_time":"2024-10-11T19:10:33.700665","exception":false,"start_time":"2024-10-11T19:10:20.853490","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-12T19:02:58.823096Z","iopub.execute_input":"2024-10-12T19:02:58.823379Z","iopub.status.idle":"2024-10-12T19:03:10.967297Z","shell.execute_reply.started":"2024-10-12T19:02:58.823346Z","shell.execute_reply":"2024-10-12T19:03:10.966330Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:03<00:00, 48986823.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-100-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\nimport numpy as np\nfrom tqdm import tqdm\n\n# Load pretrained ResNet model and modify it to act as a feature extractor\nresnet50 = models.resnet50(pretrained=True)\nresnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove the final classification layer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet50 = resnet50.to(device)\n\n# Function to extract features\ndef extract_features(dataloader, dataset_name='Dataset'):\n    resnet50.eval()  # Set model to evaluation mode\n    features = []\n    labels = []\n    with torch.no_grad():\n        for inputs, targets in tqdm(dataloader, desc=f'Extracting features from {dataset_name}', unit='batch', total=len(dataloader)):\n            outputs = resnet50(inputs.to(device)).squeeze()\n            features.append(outputs.cpu())\n            labels.append(targets)\n    return torch.vstack(features), torch.hstack(labels)\n\n\n\n# Extract features from train and test set\ntrain_features, train_labels = extract_features(trainloader, 'Train Set')\ntest_features, test_labels = extract_features(testloader, 'Test Set')","metadata":{"papermill":{"duration":106.756056,"end_time":"2024-10-11T19:12:20.464185","exception":false,"start_time":"2024-10-11T19:10:33.708129","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-12T19:03:10.968856Z","iopub.execute_input":"2024-10-12T19:03:10.969261Z","iopub.status.idle":"2024-10-12T19:04:57.920301Z","shell.execute_reply.started":"2024-10-12T19:03:10.969229Z","shell.execute_reply":"2024-10-12T19:04:57.919012Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 160MB/s] \nExtracting features from Train Set: 100%|██████████| 50/50 [01:26<00:00,  1.72s/batch]\nExtracting features from Test Set: 100%|██████████| 10/10 [00:18<00:00,  1.89s/batch]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nbatch_size = 125\n\ndef sample_negatives(train_features, train_labels, num_negatives = 10):\n    class CustomDataset(Dataset):\n        def __init__(self, features, labels):\n            self.features = features\n            self.labels = labels\n\n        def __len__(self):\n            return len(self.labels)\n\n        def __getitem__(self, idx):\n            return self.features[idx], self.labels[idx]\n\n    dataset = CustomDataset(train_features, train_labels)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    neg_indices = torch.empty((train_features.shape[0], num_negatives), dtype=torch.int)\n    \n    with tqdm(total=train_features.shape[0], desc='Sampling Negatives') as pbar:\n        for idx, (batch_features, batch_labels) in enumerate(dataloader):\n            batch_size_current = batch_features.shape[0]  # Get current batch size\n            all_indices = torch.arange(batch_size_current)\n            \n            for i in range(batch_size_current):\n                label = batch_labels[i].item()\n                neg_mask = batch_labels != label\n                \n                assert neg_mask.sum() >= 10, \"does not has enough negatives\"\n                \n                neg_candidates = all_indices[neg_mask]\n                neg_indices[i + idx * batch_size] = neg_candidates[torch.randperm(len(neg_candidates))[:num_negatives]]\n                \n                pbar.update(1)\n    \n    return neg_indices\n\nneg_indices = sample_negatives(train_features, train_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:04:57.921735Z","iopub.execute_input":"2024-10-12T19:04:57.922088Z","iopub.status.idle":"2024-10-12T19:05:01.807305Z","shell.execute_reply.started":"2024-10-12T19:04:57.922050Z","shell.execute_reply":"2024-10-12T19:05:01.806383Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Sampling Negatives: 100%|██████████| 50000/50000 [00:03<00:00, 12918.81it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass TrainDataset(Dataset):\n    def __init__(self, features, neg_indices):\n        self.features = features\n        self.neg_indices = neg_indices\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        return self.features[idx], self.neg_indices[idx]\n    \ntrain_dataset = TrainDataset(train_features, neg_indices)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:01.810028Z","iopub.execute_input":"2024-10-12T19:05:01.810339Z","iopub.status.idle":"2024-10-12T19:05:01.819298Z","shell.execute_reply.started":"2024-10-12T19:05:01.810307Z","shell.execute_reply":"2024-10-12T19:05:01.818376Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\n\nclass NeuralLSH(nn.Module):\n    def __init__(self, input_dim, hash_dim, num_tables, subset_size):\n        super(NeuralLSH, self).__init__()\n        self.input_dim = input_dim\n        self.hash_dim = hash_dim\n        self.num_tables = num_tables\n        self.subset_size = subset_size\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.powers_of_two = torch.pow(2, torch.arange(subset_size - 1, -1, -1)).float().to(device)\n        self.zero = torch.tensor([0], device=self.device)\n\n        self.init_hash_functions()\n        self.hyperplanes = nn.Parameter(torch.randn(self.input_dim, self.hash_dim, device=self.device))\n        \n    def init_hash_functions(self):\n        self.hash_functions = torch.tensor([], device=self.device).long()\n        indices = list(range(self.hash_dim))\n        for _ in range(self.num_tables):\n            random.shuffle(indices)\n            self.hash_functions = torch.cat((self.hash_functions,\n                torch.tensor([indices[:self.subset_size]], device=self.device).long()), dim=0)\n    \n    def _projection(self, features):\n        return torch.mm(features, self.hyperplanes)\n    \n    def forward(self, features):\n        return torch.tanh(self._projection(features))\n    \n    \n    def init_hash_tables(self, train_features):\n        train_features = train_features.to(self.device)\n        full_hash_codes = self._projection(train_features)\n        self.hash_tables = []\n\n        full_hash_values = torch.transpose(((full_hash_codes[:, self.hash_functions] > 0).float() @ self.powers_of_two).int(), 0, 1)\n        for table in range(self.num_tables):\n            self.hash_tables.append([])\n            for hash_val in range(2 ** self.subset_size):\n                self.hash_tables[table].append(torch.nonzero(full_hash_values[table] == hash_val).T[0].tolist())      \n\n    def get_corpus_indices(self, features):\n        features = features.to(device)\n        full_hash_codes = self._projection(features)\n        \n        full_hash_values = ((full_hash_codes[:, self.hash_functions] > 0).float() @ self.powers_of_two).int()\n                \n        corpus_indices = []\n        for hash_values in tqdm(full_hash_values, desc='Creating Corpus for Test Image', total=len(full_hash_values)):\n            indices = set()\n            for hash_table, hash_val in zip(self.hash_tables, hash_values):\n                indices.update(hash_table[hash_val.item()])\n            \n            corpus_indices.append(list(indices))\n        \n        return corpus_indices\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:01.820796Z","iopub.execute_input":"2024-10-12T19:05:01.821163Z","iopub.status.idle":"2024-10-12T19:05:01.837252Z","shell.execute_reply.started":"2024-10-12T19:05:01.821122Z","shell.execute_reply":"2024-10-12T19:05:01.836368Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def loss_func(hash_codes, neg_indices):\n    # taking alpha = beta = gamma = 1/3\n    term1 = torch.sum(torch.abs(torch.sum(hash_codes, dim=1))) / hash_codes.shape[0]\n    \n    term2 = torch.sum(torch.abs(torch.abs(hash_codes) - torch.ones(hash_codes.shape[1], device=device))) / hash_codes.shape[0]\n        \n    negs = torch.transpose(hash_codes[neg_indices], 1, 2)\n    term3 = torch.sum(torch.abs(torch.matmul(hash_codes.unsqueeze(1), negs))) / (neg_indices.shape[0] * neg_indices.shape[1])\n\n    return (term1 + term2 + term3) / 3\n\ndef train_model(train_dataloader, model, optimizer, epochs=3, device='cuda'):\n    model.to(device)\n    model.train()\n    for epoch in range(epochs):     \n        total_loss = 0\n        \n        for train_data, neg_idx in train_dataloader:\n            train_data = train_data.to(device)\n            neg_idx = neg_idx.to(device)\n\n            hash_codes = model(train_data)\n            loss = loss_func(hash_codes, neg_idx)\n            total_loss += loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        \n        if (epoch % 5 == 4 or epoch == epochs - 1):\n            print(f'Epoch {epoch+1}, Average Loss: {total_loss / len(train_dataloader)}')\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:01.838369Z","iopub.execute_input":"2024-10-12T19:05:01.838784Z","iopub.status.idle":"2024-10-12T19:05:01.851957Z","shell.execute_reply.started":"2024-10-12T19:05:01.838743Z","shell.execute_reply":"2024-10-12T19:05:01.851102Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef get_top_k_matches(train_features, test_feature, cluster_indices, k=50):\n    # Get the features of images belonging to the same cluster\n    cluster_features = train_features[cluster_indices]\n\n    # Compute cosine similarity between the test image and cluster images\n    similarities = cosine_similarity(test_feature.reshape(1, -1), cluster_features).flatten()\n    \n    # Get the top k most similar images\n    top_k_indices = np.argsort(similarities)[::-1][:k]\n    return cluster_indices[top_k_indices]","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:01.852989Z","iopub.execute_input":"2024-10-12T19:05:01.853282Z","iopub.status.idle":"2024-10-12T19:05:02.528623Z","shell.execute_reply.started":"2024-10-12T19:05:01.853249Z","shell.execute_reply":"2024-10-12T19:05:02.527615Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def precision_at_k(true_label, top_k_labels, k):\n    top_k = top_k_labels[:k]\n    correct = torch.sum(top_k == true_label).item()\n    return correct / k\n\ndef mean_average_precision(true_label, top_k_labels):\n    # Calculate precision at each rank and then compute average precision\n    precisions = []\n    correct = 0\n    for i, label in enumerate(top_k_labels):\n        if label == true_label:\n            correct += 1\n            precisions.append(correct / (i + 1))\n    return np.mean(precisions) if precisions else 0","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:02.529811Z","iopub.execute_input":"2024-10-12T19:05:02.530199Z","iopub.status.idle":"2024-10-12T19:05:02.536228Z","shell.execute_reply.started":"2024-10-12T19:05:02.530167Z","shell.execute_reply":"2024-10-12T19:05:02.535317Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_top_matches(train_features, test_features, corpus_indices):\n    # For each test image, find the top 50 matches\n    top_k_matches = []\n    for i, test_feature in tqdm(enumerate(test_features), total=len(test_features)):\n        cluster_indices = np.array(corpus_indices[i])\n\n        if (len(cluster_indices) == 0):\n            top_k_matches.append([])\n            continue\n        \n        # Get the top 50 matches based on cosine similarity\n        top_k_matches.append(get_top_k_matches(train_features, test_feature, cluster_indices))\n\n    return top_k_matches","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:02.537324Z","iopub.execute_input":"2024-10-12T19:05:02.537629Z","iopub.status.idle":"2024-10-12T19:05:02.550311Z","shell.execute_reply.started":"2024-10-12T19:05:02.537598Z","shell.execute_reply":"2024-10-12T19:05:02.549485Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def evaluate(train_labels, test_labels, top_k_matches):\n    # Evaluate for all test images\n    precision_10 = []\n    precision_50 = []\n    mean_ap = []\n    for i, matches in enumerate(top_k_matches):\n    # for i, matches in tqdm(enumerate(top_k_matches), desc='Evaluating Metrics', unit='image', total=len(top_k_matches)):\n        true_label = test_labels[i]\n        matched_labels = train_labels[matches]\n        \n        precision_10.append(precision_at_k(true_label, matched_labels, 10))\n        precision_50.append(precision_at_k(true_label, matched_labels, 50))\n        mean_ap.append(mean_average_precision(true_label, matched_labels))\n\n    return np.mean(precision_10), np.mean(precision_50), np.mean(mean_ap)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:02.552974Z","iopub.execute_input":"2024-10-12T19:05:02.553288Z","iopub.status.idle":"2024-10-12T19:05:02.560262Z","shell.execute_reply.started":"2024-10-12T19:05:02.553252Z","shell.execute_reply":"2024-10-12T19:05:02.559363Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"num_features = train_features.shape[1]\nhash_dim = 16\nnum_tables = 10\nsubset_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:02.561289Z","iopub.execute_input":"2024-10-12T19:05:02.561589Z","iopub.status.idle":"2024-10-12T19:05:02.570791Z","shell.execute_reply.started":"2024-10-12T19:05:02.561558Z","shell.execute_reply":"2024-10-12T19:05:02.569810Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nmodel = NeuralLSH(num_features, hash_dim, num_tables, subset_size)\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\ntrain_model(train_dataloader, model, optimizer, 40)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:02.571870Z","iopub.execute_input":"2024-10-12T19:05:02.572140Z","iopub.status.idle":"2024-10-12T19:05:52.328195Z","shell.execute_reply.started":"2024-10-12T19:05:02.572110Z","shell.execute_reply":"2024-10-12T19:05:52.327181Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 5, Average Loss: 1.8775205612182617\nEpoch 10, Average Loss: 1.8193511962890625\nEpoch 15, Average Loss: 1.7734841108322144\nEpoch 20, Average Loss: 1.7543021440505981\nEpoch 25, Average Loss: 1.7264974117279053\nEpoch 30, Average Loss: 1.7076013088226318\nEpoch 35, Average Loss: 1.6904443502426147\nEpoch 40, Average Loss: 1.6748626232147217\n","output_type":"stream"}]},{"cell_type":"code","source":"model.init_hash_tables(train_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:52.331915Z","iopub.execute_input":"2024-10-12T19:05:52.332237Z","iopub.status.idle":"2024-10-12T19:05:52.725595Z","shell.execute_reply.started":"2024-10-12T19:05:52.332205Z","shell.execute_reply":"2024-10-12T19:05:52.724769Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"corpus_indices = model.get_corpus_indices(test_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:52.726824Z","iopub.execute_input":"2024-10-12T19:05:52.727140Z","iopub.status.idle":"2024-10-12T19:05:57.028346Z","shell.execute_reply.started":"2024-10-12T19:05:52.727108Z","shell.execute_reply":"2024-10-12T19:05:57.027405Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Creating Corpus for Test Image: 100%|██████████| 10000/10000 [00:04<00:00, 2338.58it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"top_matches = get_top_matches(train_features, test_features, corpus_indices)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:05:57.029580Z","iopub.execute_input":"2024-10-12T19:05:57.029975Z","iopub.status.idle":"2024-10-12T19:11:51.923137Z","shell.execute_reply.started":"2024-10-12T19:05:57.029935Z","shell.execute_reply":"2024-10-12T19:11:51.921960Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 10000/10000 [05:54<00:00, 28.18it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"precision_10, precision_50, mean_ap = evaluate(train_labels, test_labels, top_matches)\n\nprint(f'Mean Precision@10: {precision_10:.4f}')\nprint(f'Mean Precision@50: {precision_50:.4f}')\nprint(f'Mean Average Precision: {mean_ap:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-10-12T19:11:51.924883Z","iopub.execute_input":"2024-10-12T19:11:51.925719Z","iopub.status.idle":"2024-10-12T19:11:55.852085Z","shell.execute_reply.started":"2024-10-12T19:11:51.925655Z","shell.execute_reply":"2024-10-12T19:11:55.850892Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Mean Precision@10: 0.4610\nMean Precision@50: 0.3499\nMean Average Precision: 0.4934\n","output_type":"stream"}]}]}