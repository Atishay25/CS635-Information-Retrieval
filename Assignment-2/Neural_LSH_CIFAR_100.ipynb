{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5a0dba",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-12T16:33:38.775384Z",
     "iopub.status.busy": "2024-10-12T16:33:38.774947Z",
     "iopub.status.idle": "2024-10-12T16:33:53.285339Z",
     "shell.execute_reply": "2024-10-12T16:33:53.284318Z"
    },
    "papermill": {
     "duration": 14.520471,
     "end_time": "2024-10-12T16:33:53.289065",
     "exception": false,
     "start_time": "2024-10-12T16:33:38.768594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:05<00:00, 29481148.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transforms for data augmentation and normalization\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                               ])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "num_labels = 100\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39c7c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:33:53.323565Z",
     "iopub.status.busy": "2024-10-12T16:33:53.322280Z",
     "iopub.status.idle": "2024-10-12T16:35:40.531017Z",
     "shell.execute_reply": "2024-10-12T16:35:40.529764Z"
    },
    "papermill": {
     "duration": 107.224088,
     "end_time": "2024-10-12T16:35:40.533800",
     "exception": false,
     "start_time": "2024-10-12T16:33:53.309712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 188MB/s]\n",
      "Extracting features from Train Set: 100%|██████████| 50/50 [01:26<00:00,  1.73s/batch]\n",
      "Extracting features from Test Set: 100%|██████████| 10/10 [00:19<00:00,  1.90s/batch]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pretrained ResNet model and modify it to act as a feature extractor\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50 = nn.Sequential(*list(resnet50.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50 = resnet50.to(device)\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(dataloader, dataset_name='Dataset'):\n",
    "    resnet50.eval()  # Set model to evaluation mode\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=f'Extracting features from {dataset_name}', unit='batch', total=len(dataloader)):\n",
    "            outputs = resnet50(inputs.to(device)).squeeze()\n",
    "            features.append(outputs.cpu())\n",
    "            labels.append(targets)\n",
    "    return torch.vstack(features), torch.hstack(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Extract features from train and test set\n",
    "train_features, train_labels = extract_features(trainloader, 'Train Set')\n",
    "test_features, test_labels = extract_features(testloader, 'Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4e3d89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:40.565118Z",
     "iopub.status.busy": "2024-10-12T16:35:40.564723Z",
     "iopub.status.idle": "2024-10-12T16:35:44.551877Z",
     "shell.execute_reply": "2024-10-12T16:35:44.550853Z"
    },
    "papermill": {
     "duration": 4.005187,
     "end_time": "2024-10-12T16:35:44.553873",
     "exception": false,
     "start_time": "2024-10-12T16:35:40.548686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Negatives: 100%|██████████| 50000/50000 [00:03<00:00, 12589.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 125\n",
    "\n",
    "def sample_negatives(train_features, train_labels, num_negatives = 10):\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, features, labels):\n",
    "            self.features = features\n",
    "            self.labels = labels\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.features[idx], self.labels[idx]\n",
    "\n",
    "    dataset = CustomDataset(train_features, train_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    neg_indices = torch.empty((train_features.shape[0], num_negatives), dtype=torch.int)\n",
    "    \n",
    "    with tqdm(total=train_features.shape[0], desc='Sampling Negatives') as pbar:\n",
    "        for idx, (batch_features, batch_labels) in enumerate(dataloader):\n",
    "            batch_size_current = batch_features.shape[0]  # Get current batch size\n",
    "            all_indices = torch.arange(batch_size_current)\n",
    "            \n",
    "            for i in range(batch_size_current):\n",
    "                label = batch_labels[i].item()\n",
    "                neg_mask = batch_labels != label\n",
    "                \n",
    "                assert neg_mask.sum() >= 10, \"does not has enough negatives\"\n",
    "                \n",
    "                neg_candidates = all_indices[neg_mask]\n",
    "                neg_indices[i + idx * batch_size] = neg_candidates[torch.randperm(len(neg_candidates))[:num_negatives]]\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    return neg_indices\n",
    "\n",
    "neg_indices = sample_negatives(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee12c012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:44.590991Z",
     "iopub.status.busy": "2024-10-12T16:35:44.590649Z",
     "iopub.status.idle": "2024-10-12T16:35:44.597282Z",
     "shell.execute_reply": "2024-10-12T16:35:44.596414Z"
    },
    "papermill": {
     "duration": 0.027311,
     "end_time": "2024-10-12T16:35:44.599094",
     "exception": false,
     "start_time": "2024-10-12T16:35:44.571783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, features, neg_indices):\n",
    "        self.features = features\n",
    "        self.neg_indices = neg_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.neg_indices[idx]\n",
    "    \n",
    "train_dataset = TrainDataset(train_features, neg_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43dc200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:44.636459Z",
     "iopub.status.busy": "2024-10-12T16:35:44.635634Z",
     "iopub.status.idle": "2024-10-12T16:35:44.652528Z",
     "shell.execute_reply": "2024-10-12T16:35:44.651551Z"
    },
    "papermill": {
     "duration": 0.037598,
     "end_time": "2024-10-12T16:35:44.654394",
     "exception": false,
     "start_time": "2024-10-12T16:35:44.616796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NeuralLSH(nn.Module):\n",
    "    def __init__(self, input_dim, hash_dim, num_tables, subset_size):\n",
    "        super(NeuralLSH, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hash_dim = hash_dim\n",
    "        self.num_tables = num_tables\n",
    "        self.subset_size = subset_size\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.powers_of_two = torch.pow(2, torch.arange(subset_size - 1, -1, -1)).float().to(device)\n",
    "        self.zero = torch.tensor([0], device=self.device)\n",
    "\n",
    "        self.init_hash_functions()\n",
    "        self.hyperplanes = nn.Parameter(torch.randn(self.input_dim, self.hash_dim, device=self.device))\n",
    "        \n",
    "    def init_hash_functions(self):\n",
    "        self.hash_functions = torch.tensor([], device=self.device).long()\n",
    "        indices = list(range(self.hash_dim))\n",
    "        for _ in range(self.num_tables):\n",
    "            random.shuffle(indices)\n",
    "            self.hash_functions = torch.cat((self.hash_functions,\n",
    "                torch.tensor([indices[:self.subset_size]], device=self.device).long()), dim=0)\n",
    "    \n",
    "    def _projection(self, features):\n",
    "        return torch.mm(features, self.hyperplanes)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        return torch.tanh(self._projection(features))\n",
    "    \n",
    "    \n",
    "    def init_hash_tables(self, train_features):\n",
    "        train_features = train_features.to(self.device)\n",
    "        full_hash_codes = self._projection(train_features)\n",
    "        self.hash_tables = []\n",
    "\n",
    "        full_hash_values = torch.transpose(((full_hash_codes[:, self.hash_functions] > 0).float() @ self.powers_of_two).int(), 0, 1)\n",
    "        for table in range(self.num_tables):\n",
    "            self.hash_tables.append([])\n",
    "            for hash_val in range(2 ** self.subset_size):\n",
    "                self.hash_tables[table].append(torch.nonzero(full_hash_values[table] == hash_val).T[0].tolist())      \n",
    "\n",
    "    def get_corpus_indices(self, features):\n",
    "        features = features.to(device)\n",
    "        full_hash_codes = self._projection(features)\n",
    "        \n",
    "        full_hash_values = ((full_hash_codes[:, self.hash_functions] > 0).float() @ self.powers_of_two).int()\n",
    "                \n",
    "        corpus_indices = []\n",
    "        for hash_values in tqdm(full_hash_values, desc='Creating Corpus for Test Image', total=len(full_hash_values)):\n",
    "            indices = set()\n",
    "            for hash_table, hash_val in zip(self.hash_tables, hash_values):\n",
    "                if (type(hash_table[hash_val.item()]) != list):\n",
    "                    return hash_table[hash_val.item()]\n",
    "                indices.update(hash_table[hash_val.item()])\n",
    "            \n",
    "            corpus_indices.append(list(indices))\n",
    "        \n",
    "        return corpus_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7e55d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:44.692901Z",
     "iopub.status.busy": "2024-10-12T16:35:44.692537Z",
     "iopub.status.idle": "2024-10-12T16:35:44.702482Z",
     "shell.execute_reply": "2024-10-12T16:35:44.701592Z"
    },
    "papermill": {
     "duration": 0.031405,
     "end_time": "2024-10-12T16:35:44.704358",
     "exception": false,
     "start_time": "2024-10-12T16:35:44.672953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_func(hash_codes, neg_indices):\n",
    "    # taking alpha = beta = gamma = 1/3\n",
    "    term1 = torch.sum(torch.abs(torch.sum(hash_codes, dim=1))) / hash_codes.shape[0]\n",
    "    \n",
    "    term2 = torch.sum(torch.abs(torch.abs(hash_codes) - torch.ones(hash_codes.shape[1], device=device))) / hash_codes.shape[0]\n",
    "        \n",
    "    negs = torch.transpose(hash_codes[neg_indices], 1, 2)\n",
    "    term3 = torch.sum(torch.abs(torch.matmul(hash_codes.unsqueeze(1), negs))) / (neg_indices.shape[0] * neg_indices.shape[1])\n",
    "\n",
    "    return (term1 + term2 + term3) / 3\n",
    "\n",
    "def train_model(train_dataloader, model, optimizer, epochs=3, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):     \n",
    "        total_loss = 0\n",
    "        \n",
    "        for train_data, neg_idx in train_dataloader:\n",
    "            train_data = train_data.to(device)\n",
    "            neg_idx = neg_idx.to(device)\n",
    "\n",
    "            hash_codes = model(train_data)\n",
    "            loss = loss_func(hash_codes, neg_idx)\n",
    "            total_loss += loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        if (epoch % 5 == 4 or epoch == epochs - 1):\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss / len(train_dataloader)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33acc939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:44.742309Z",
     "iopub.status.busy": "2024-10-12T16:35:44.741723Z",
     "iopub.status.idle": "2024-10-12T16:35:46.022235Z",
     "shell.execute_reply": "2024-10-12T16:35:46.021361Z"
    },
    "papermill": {
     "duration": 1.302142,
     "end_time": "2024-10-12T16:35:46.024609",
     "exception": false,
     "start_time": "2024-10-12T16:35:44.722467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_top_k_matches(train_features, test_feature, cluster_indices, k=50):\n",
    "    # Get the features of images belonging to the same cluster\n",
    "    cluster_features = train_features[cluster_indices]\n",
    "\n",
    "    # Compute cosine similarity between the test image and cluster images\n",
    "    similarities = cosine_similarity(test_feature.reshape(1, -1), cluster_features).flatten()\n",
    "    \n",
    "    # Get the top k most similar images\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return cluster_indices[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36044a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:46.063260Z",
     "iopub.status.busy": "2024-10-12T16:35:46.062495Z",
     "iopub.status.idle": "2024-10-12T16:35:46.068967Z",
     "shell.execute_reply": "2024-10-12T16:35:46.068136Z"
    },
    "papermill": {
     "duration": 0.028094,
     "end_time": "2024-10-12T16:35:46.070992",
     "exception": false,
     "start_time": "2024-10-12T16:35:46.042898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_at_k(true_label, top_k_labels, k):\n",
    "    top_k = top_k_labels[:k]\n",
    "    correct = torch.sum(top_k == true_label).item()\n",
    "    return correct / k\n",
    "\n",
    "def mean_average_precision(true_label, top_k_labels):\n",
    "    # Calculate precision at each rank and then compute average precision\n",
    "    precisions = []\n",
    "    correct = 0\n",
    "    for i, label in enumerate(top_k_labels):\n",
    "        if label == true_label:\n",
    "            correct += 1\n",
    "            precisions.append(correct / (i + 1))\n",
    "    return np.mean(precisions) if precisions else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fcb68e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:46.108665Z",
     "iopub.status.busy": "2024-10-12T16:35:46.108007Z",
     "iopub.status.idle": "2024-10-12T16:35:46.113968Z",
     "shell.execute_reply": "2024-10-12T16:35:46.113148Z"
    },
    "papermill": {
     "duration": 0.026896,
     "end_time": "2024-10-12T16:35:46.115779",
     "exception": false,
     "start_time": "2024-10-12T16:35:46.088883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_matches(train_features, test_features, corpus_indices):\n",
    "    # For each test image, find the top 50 matches\n",
    "    top_k_matches = []\n",
    "    for i, test_feature in tqdm(enumerate(test_features), total=len(test_features)):\n",
    "        cluster_indices = np.array(corpus_indices[i])\n",
    "\n",
    "        if (len(cluster_indices) == 0):\n",
    "            top_k_matches.append([])\n",
    "            continue\n",
    "        \n",
    "        # Get the top 50 matches based on cosine similarity\n",
    "        top_k_matches.append(get_top_k_matches(train_features, test_feature, cluster_indices))\n",
    "\n",
    "    return top_k_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb7ddc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:46.154196Z",
     "iopub.status.busy": "2024-10-12T16:35:46.153502Z",
     "iopub.status.idle": "2024-10-12T16:35:46.160298Z",
     "shell.execute_reply": "2024-10-12T16:35:46.159386Z"
    },
    "papermill": {
     "duration": 0.028523,
     "end_time": "2024-10-12T16:35:46.162275",
     "exception": false,
     "start_time": "2024-10-12T16:35:46.133752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(train_labels, test_labels, top_k_matches):\n",
    "    # Evaluate for all test images\n",
    "    precision_10 = []\n",
    "    precision_50 = []\n",
    "    mean_ap = []\n",
    "    for i, matches in enumerate(top_k_matches):\n",
    "    # for i, matches in tqdm(enumerate(top_k_matches), desc='Evaluating Metrics', unit='image', total=len(top_k_matches)):\n",
    "        true_label = test_labels[i]\n",
    "        matched_labels = train_labels[matches]\n",
    "        \n",
    "        precision_10.append(precision_at_k(true_label, matched_labels, 10))\n",
    "        precision_50.append(precision_at_k(true_label, matched_labels, 50))\n",
    "        mean_ap.append(mean_average_precision(true_label, matched_labels))\n",
    "\n",
    "    return np.mean(precision_10), np.mean(precision_50), np.mean(mean_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620c22c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:46.201072Z",
     "iopub.status.busy": "2024-10-12T16:35:46.200639Z",
     "iopub.status.idle": "2024-10-12T16:35:46.204901Z",
     "shell.execute_reply": "2024-10-12T16:35:46.204042Z"
    },
    "papermill": {
     "duration": 0.026315,
     "end_time": "2024-10-12T16:35:46.206811",
     "exception": false,
     "start_time": "2024-10-12T16:35:46.180496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = train_features.shape[1]\n",
    "hash_dim = 16\n",
    "num_tables = 10\n",
    "subset_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42177087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:35:46.244702Z",
     "iopub.status.busy": "2024-10-12T16:35:46.243952Z",
     "iopub.status.idle": "2024-10-12T16:36:37.442614Z",
     "shell.execute_reply": "2024-10-12T16:36:37.441628Z"
    },
    "papermill": {
     "duration": 51.220066,
     "end_time": "2024-10-12T16:36:37.444929",
     "exception": false,
     "start_time": "2024-10-12T16:35:46.224863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 1.872665524482727\n",
      "Epoch 10, Average Loss: 1.8060702085494995\n",
      "Epoch 15, Average Loss: 1.772876262664795\n",
      "Epoch 20, Average Loss: 1.7406940460205078\n",
      "Epoch 25, Average Loss: 1.7187819480895996\n",
      "Epoch 30, Average Loss: 1.6969380378723145\n",
      "Epoch 35, Average Loss: 1.6824613809585571\n",
      "Epoch 40, Average Loss: 1.6680251359939575\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = NeuralLSH(num_features, hash_dim, num_tables, subset_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_model(train_dataloader, model, optimizer, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "220934f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:36:37.484613Z",
     "iopub.status.busy": "2024-10-12T16:36:37.484235Z",
     "iopub.status.idle": "2024-10-12T16:36:37.900232Z",
     "shell.execute_reply": "2024-10-12T16:36:37.899171Z"
    },
    "papermill": {
     "duration": 0.438527,
     "end_time": "2024-10-12T16:36:37.902678",
     "exception": false,
     "start_time": "2024-10-12T16:36:37.464151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.init_hash_tables(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de96de4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:36:37.942556Z",
     "iopub.status.busy": "2024-10-12T16:36:37.942182Z",
     "iopub.status.idle": "2024-10-12T16:36:43.259926Z",
     "shell.execute_reply": "2024-10-12T16:36:43.258897Z"
    },
    "papermill": {
     "duration": 5.340258,
     "end_time": "2024-10-12T16:36:43.262046",
     "exception": false,
     "start_time": "2024-10-12T16:36:37.921788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Corpus for Test Image: 100%|██████████| 10000/10000 [00:05<00:00, 1889.85it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = model.get_corpus_indices(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "609b48b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:36:43.309440Z",
     "iopub.status.busy": "2024-10-12T16:36:43.309025Z",
     "iopub.status.idle": "2024-10-12T16:42:31.991782Z",
     "shell.execute_reply": "2024-10-12T16:42:31.990477Z"
    },
    "papermill": {
     "duration": 348.710609,
     "end_time": "2024-10-12T16:42:31.995577",
     "exception": false,
     "start_time": "2024-10-12T16:36:43.284968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:48<00:00, 28.69it/s]\n"
     ]
    }
   ],
   "source": [
    "top_matches = get_top_matches(train_features, test_features, corpus_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "181e8b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T16:42:32.520636Z",
     "iopub.status.busy": "2024-10-12T16:42:32.520253Z",
     "iopub.status.idle": "2024-10-12T16:42:36.555134Z",
     "shell.execute_reply": "2024-10-12T16:42:36.554084Z"
    },
    "papermill": {
     "duration": 4.258468,
     "end_time": "2024-10-12T16:42:36.557197",
     "exception": false,
     "start_time": "2024-10-12T16:42:32.298729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Precision@10: 0.4668\n",
      "Mean Precision@50: 0.3583\n",
      "Mean Average Precision: 0.4983\n"
     ]
    }
   ],
   "source": [
    "precision_10, precision_50, mean_ap = evaluate(train_labels, test_labels, top_matches)\n",
    "\n",
    "print(f'Mean Precision@10: {precision_10:.4f}')\n",
    "print(f'Mean Precision@50: {precision_50:.4f}')\n",
    "print(f'Mean Average Precision: {mean_ap:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 542.844258,
   "end_time": "2024-10-12T16:42:38.804207",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-12T16:33:35.959949",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
